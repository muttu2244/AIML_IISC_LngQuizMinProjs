{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aisha-partha/AIMLOps-Assignments/blob/main/M5_AST_01_TextPreprocessing_using_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A programme by IISc and TalentSprint\n",
        "### Assignment 1: Text Preprocessing using spaCy"
      ],
      "metadata": {
        "id": "zSstSEIv155v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the spaCy library\n",
        "* perform simple natural language processing tasks using the spaCy library"
      ],
      "metadata": {
        "id": "akVZjBjD2PtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "VGel018GxQGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spaCy** is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "\n",
        "It is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
        "\n",
        "spaCy's features and capabilities include:\n",
        "\n",
        "- ***Tokenization***:\tSegmenting text into words, punctuations marks etc.\n",
        "- ***Part-of-speech (POS) Tagging***: Assigning word types to tokens, like verb or noun.\n",
        "- ***Dependency Parsing***: Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
        "- ***Lemmatization***: Assigning the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.\n",
        "- ***Sentence Boundary Detection (SBD)***: Finding and segmenting individual sentences.\n",
        "- ***Named Entity Recognition (NER)***: Labelling named “real-world” objects, like persons, companies or locations.\n",
        "- ***Entity Linking (EL)***: Disambiguating textual entities to unique identifiers in a knowledge base.\n",
        "- ***Similarity***: Comparing words, text spans and documents and how similar they are to each other.\n",
        "- ***Text Classification***: Assigning categories or labels to a whole document, or parts of a document.\n",
        "- ***Rule-based Matching***: Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
        "- ***Training***: Updating and improving a statistical model's predictions.\n",
        "- ***Serialization***: Saving objects to files or byte strings.\n"
      ],
      "metadata": {
        "id": "TI_iPJtzxSEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical models\n",
        "\n",
        "While some of spaCy's features work independently, others require ***trained pipelines*** to be loaded, which enable spaCy to predict linguistic annotations - for example, whether a word is a verb or a noun.\n",
        "\n",
        "A trained pipeline can consist of multiple components that use a statistical model trained on labeled data.\n",
        "\n",
        "spaCy currently offers trained pipelines for a variety of languages, which can be installed as individual Python modules. Pipeline packages can differ in size, speed, memory usage, accuracy and the data they include.\n",
        "\n",
        "For English language, available trained pipelines include:\n",
        "- `en_core_web_sm`\n",
        "- `en_core_web_md`\n",
        "- `en_core_web_lg`\n",
        "- `en_core_web_trf` - English transformer pipeline\n",
        "\n",
        "To know more about trained pipelines for English, refer [here](https://spacy.io/models/en).\n",
        "\n",
        "Let's perform basic NLP tasks with spaCy using an English trained pipeline.\n"
      ],
      "metadata": {
        "id": "k9SWBYOydg-e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2304896\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9916583736\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "WBPPuGmBlDIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd3f839d-dded-4cff-9f43-eb9a225233c1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2304896&recordId=5774\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M5_AST_01_TextPreprocessing_using_spaCy\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install packages"
      ],
      "metadata": {
        "id": "ysivBdlXMxoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install spacy==3.7.4"
      ],
      "metadata": {
        "id": "RnmSjKExJ-AK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664a9077-23b2-4906-f6e5-714584bd4370"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "id": "NlHYHxmhJWqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff84a269-3a61-4f23-b5cd-c1f0c2e31254"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.7.4                         \n",
            "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
            "Platform         Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Python version   3.10.12                       \n",
            "Pipelines        en_core_web_sm (3.7.1)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above info, we can see that by default spaCy contains the small trained pipeline for English `en_core_web_sm`.\n",
        "\n",
        "To use medium, large, and transformer trained pipelines, they need to be installed first using the `!python -m spacy download` command.\n",
        "\n",
        "For example: `!python -m spacy download en_core_web_trf`"
      ],
      "metadata": {
        "id": "ioo8J8kk270t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install English transformer pipeline\n",
        "# NOTE that Runtime needs to restart after this step\n",
        "\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "enIOTr8y6hf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f092e2c2-6ff9-4eb0-9714-525a1e194a5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-trf==3.7.3\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.3/en_core_web_trf-3.7.3-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-trf==3.7.3) (3.7.4)\n",
            "Collecting spacy-curated-transformers<0.3.0,>=0.2.0 (from en-core-web-trf==3.7.3)\n",
            "  Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.26.4)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading curated_tokenizers-0.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.3.1+cu121)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.10/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.5.15)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.3.0)\n",
            "Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_tokenizers-0.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (731 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, curated-tokenizers, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, curated-transformers, spacy-curated-transformers, en-core-web-trf\n",
            "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.7.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 spacy-curated-transformers-0.2.2\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the Runtime/Session**"
      ],
      "metadata": {
        "id": "HpB5KKWSKkby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "id": "x83z3TOfNGS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f444c97b-719f-4e1f-8a40-c7aef3d84aad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.7.4                         \n",
            "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
            "Platform         Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Python version   3.10.12                       \n",
            "Pipelines        en_core_web_trf (3.7.3), en_core_web_sm (3.7.1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages"
      ],
      "metadata": {
        "id": "d6wPl_vwqL6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FFOiNNeP0giN"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the trained pipeline\n",
        "\n",
        "Once you've downloaded and installed a trained pipeline, you can load it via `spacy.load()`. This will return a *Language object* containing all components and data needed to process text. We usually call it `nlp`.\n"
      ],
      "metadata": {
        "id": "OoRmCXB55X3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load transformer pipeline for English\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "# This gives us a Language object\n",
        "nlp"
      ],
      "metadata": {
        "id": "GGlQY8Ro2hw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479226e1-72f6-471c-f7d8-c7700ac6139a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7946f2c8d570>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esentially, spaCy's *Language* object is a pipeline that uses the language model to perform a number of natural language processing tasks such as *tokenization*, *part-of-speech tagging*, *syntactic parsing*, *named entity recognition*, etc.\n",
        "\n",
        "<br>\n",
        "<img src='https://cdn.iisc.talentsprint.com/AIandMLOps/Images/spacy_pipeline.png' width=800px>\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "UeGDyVaDpczn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline names\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "id": "grxmbI2H6H5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1b2f6e-9b1e-406a-d592-611745d5e344"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing basic NLP tasks using spaCy"
      ],
      "metadata": {
        "id": "cVztM7aSHg-N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWwX8KlNK3NX"
      },
      "source": [
        "Calling the Language object, `nlp`, on a string of text will return a processed *Doc*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QLEKXAxxK3NX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa5369f5-5c33-4ad9-98ad-7f2ab569c4bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Apple is looking at buying U.K. startup for $1 billion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# An example sentence\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "beDKSs4XK3NY"
      },
      "outputs": [],
      "source": [
        "# Feed the string object under 'text' to the Language object under 'nlp'\n",
        "# Store the result under the variable 'doc'\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc)"
      ],
      "metadata": {
        "id": "-d28toejq29Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d59a39-149e-4b63-ac36-32664e62720c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xDMhpziK3NX"
      },
      "source": [
        "Passing the variable `text` to the _Language_ object `nlp` returns a spaCy *Doc* object, short for document.\n",
        "\n",
        "This object contains both the input text stored under `text` and the results of natural language processing using spaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "69rWTI8oK3NY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57544cc5-bd94-47d1-ae6a-1edc0be6b99c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Apple is looking at buying U.K. startup for $1 billion."
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Call the variable to examine the object\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLU_RXJuK3NZ"
      },
      "source": [
        "Calling the variable `doc` returns the contents of the object.\n",
        "\n",
        "Although the output resembles that of a Python string, the *Doc* object contains a wealth of information about its linguistic structure, which spaCy generated by passing the text through the NLP pipeline.\n",
        "\n",
        "Let's examine the tasks that were performed under the hood after the input sentence was provided to the language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhcEc6AiK3NZ"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybAUeP7dK3Na"
      },
      "source": [
        "*Tokenization* breaks the text down into words, punctuation and so on.\n",
        "\n",
        "The diagram below outlines the tasks that spaCy can perform after a text has been tokenised, such as *part-of-speech tagging*, *syntactic parsing* and *named entity recognition*.\n",
        "\n",
        "<img src='https://cdn.iisc.talentsprint.com/AIandMLOps/Images/spacy_pipeline.png' width=800px>\n",
        "\n",
        "Each *Doc* consists of individual tokens, and we can iterate over them.\n",
        "\n",
        "Let's print out each *Token* object stored in the _Doc_ object `doc`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OxUEyrg8K3Nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ce3fa5-0261-42a0-cb7f-df53e4df64d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token\n",
            "====================\n",
            "Apple\n",
            "is\n",
            "looking\n",
            "at\n",
            "buying\n",
            "U.K.\n",
            "startup\n",
            "for\n",
            "$\n",
            "1\n",
            "billion\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "# Tokens present inside the document\n",
        "\n",
        "print(\"Token\\n\"+'='*20)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KJaYn-3K3Nc"
      },
      "source": [
        "### Part-of-speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnGIsQRHK3Nd"
      },
      "source": [
        "Part-of-speech (POS) tagging is the task of determining the word class of a token. This is crucial for *disambiguation*, because different parts of speech may have similar forms.\n",
        "\n",
        ">Consider the example: *The sailor dogs the hatch*.<br>\n",
        ">The present tense of the verb *dog* (to fasten something with something) is precisely the same as the plural form of the noun *dog*: *dogs*.\n",
        "\n",
        "To identify the correct word class, we must examine the context in which the word appears.\n",
        "\n",
        "spaCy provides two types of part-of-speech tags, coarse and fine-grained, which are stored under the attributes `pos_` and `tag_`, respectively.\n",
        "\n",
        "To access the results of POS tagging, let's loop over the *Doc* object `doc` and print each *Token* and its part-of-speech tags."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the token and the POS tags\n",
        "\n",
        "print(f\"{' ':<30}POS tag\\n{' ':<20}{'-'*25}\")\n",
        "print(f\"{'Token':<20}{'Coarse':<13}Fine-grained\\n{'='*45}\")\n",
        "\n",
        "for token in doc:\n",
        "    coarse = token.pos_         # coarse pos tag\n",
        "    fine = token.tag_           # fine-grained pos tag\n",
        "\n",
        "    print(f\"{token.text:<20}{coarse:<13}{fine}\")"
      ],
      "metadata": {
        "id": "Va5tmZcItRvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "789a19f3-c116-4cc5-8a2b-ad0656e602c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              POS tag\n",
            "                    -------------------------\n",
            "Token               Coarse       Fine-grained\n",
            "=============================================\n",
            "Apple               PROPN        NNP\n",
            "is                  AUX          VBZ\n",
            "looking             VERB         VBG\n",
            "at                  ADP          IN\n",
            "buying              VERB         VBG\n",
            "U.K.                PROPN        NNP\n",
            "startup             NOUN         NN\n",
            "for                 ADP          IN\n",
            "$                   SYM          $\n",
            "1                   NUM          CD\n",
            "billion             NUM          CD\n",
            ".                   PUNCT        .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8u4RTfgK3Ny"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcxwD63HK3Nz"
      },
      "source": [
        "A **lemma** is the base form of a word.\n",
        "\n",
        "Unless explicitly instructed, computers cannot tell the difference between singular and plural forms of words, but treat them as distinct tokens, because their forms differ.\n",
        "\n",
        "For instance, if we want to count the occurrences of words, a process known as _lemmatization_ is needed to group together the different forms of the same token.\n",
        "\n",
        "Lemmas are available for each _Token_ under the attribute `lemma_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JtMeYKd4K3Nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b80081-94a7-47b5-d8fe-e14f0ca3d944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token                Lemma\n",
            "==============================\n",
            "Apple                Apple\n",
            "is                   be\n",
            "looking              look\n",
            "at                   at\n",
            "buying               buy\n",
            "U.K.                 U.K.\n",
            "startup              startup\n",
            "for                  for\n",
            "$                    $\n",
            "1                    1\n",
            "billion              billion\n",
            ".                    .\n"
          ]
        }
      ],
      "source": [
        "# Print the token and its base form\n",
        "\n",
        "print(f\"{'Token':<20} Lemma\\n{'='*30}\")\n",
        "\n",
        "for token in doc:\n",
        "    lemma = token.lemma_\n",
        "    print(f\"{token.text:<20} {lemma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing punctuations, stop words, converting to lowercase"
      ],
      "metadata": {
        "id": "qnstEWPGR_5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop words in spacy\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "print(len(spacy_stopwords))\n",
        "print(spacy_stopwords)"
      ],
      "metadata": {
        "id": "jPZjVnY-SUET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282fb4de-3695-460b-85ca-f08135b3f748"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "{'‘ve', 'indeed', 'take', 'her', '‘s', 'as', 'these', 'hereupon', 'whence', 'thru', 'forty', 'had', 'part', 'wherever', 'first', 'too', 'various', 'never', 'will', 'few', 'most', 'was', 'no', \"'s\", 'almost', 'with', '’d', 'many', 'onto', 'somehow', 'were', 'am', 'down', 'hers', 'latterly', 'of', 'very', 'seemed', 'then', 'via', 'whole', 'up', 'himself', 'it', 'above', 'my', 'less', 'two', 'when', 'move', 'although', 'without', 'that', 'wherein', 'top', 'elsewhere', 'third', 'until', '’ll', 'beyond', 'sometime', '’ve', 'but', 'an', 'there', 'amongst', 'same', 'not', 'therefore', 'they', 'also', 'whose', 'me', 'at', 'last', 'someone', 'them', 'something', 'put', 'what', 'make', 'once', 'whether', 'your', 'often', 're', 'see', 'alone', 'out', 'seeming', 'fifteen', 'every', 'who', 'quite', 'anyway', 'which', 'full', 'over', 'least', 'their', 'along', 'behind', 'a', 'have', 'towards', 'still', 'i', 'some', 'regarding', 'five', 'again', 'anywhere', 'can', 'noone', 'and', 'because', 'made', 'into', 'amount', \"n't\", 'from', 'nor', 'fifty', \"'d\", 'did', 'hence', 'several', 'show', 'hereafter', 'whereafter', 'yourselves', 'yourself', 'next', 'now', 'eleven', 'here', 'neither', 'otherwise', 'more', 'together', 'bottom', 'against', 'during', 'always', 'across', 'somewhere', 'besides', 'under', 'our', 'hereby', 'yet', 'side', 'to', 'per', 'everything', 'own', 'toward', 'whoever', 'unless', 'became', 'whatever', 'moreover', 'upon', 'does', 'none', 'keep', 'however', 'name', 'three', '‘d', 'itself', 'everyone', 'he', 'would', 'among', '’m', 'twenty', 'rather', 'thus', 'being', 'where', 'herself', 'herein', 'how', 'latter', 'only', \"'re\", 'n’t', '‘ll', 'else', 'ca', 'afterwards', 'due', '‘re', 'after', 'say', 'becomes', 'enough', 'ourselves', 'this', 'please', 'has', 'eight', 'either', 'further', 'the', 'sometimes', 'beside', 'used', '‘m', '’s', 'might', 'twelve', 'whereby', 'anyhow', 'one', 'she', 'thereupon', 'why', 'go', 'so', 'both', 'whereas', 'through', 'serious', 'much', 'beforehand', '’re', 'any', 'do', 'yours', 'another', 'ours', 'below', 'former', 'such', 'nine', 'just', 'all', 'could', 'between', 'myself', 'other', 'anything', 'cannot', 'we', 'whereupon', 'or', 'except', 'within', 'before', 'on', 'should', 'seems', 'nowhere', 'for', 'really', 'him', 'perhaps', 'whom', 'already', 'formerly', 'back', 'about', 'off', 'six', 'done', 'call', 'using', 'though', 'four', 'ten', 'whither', 'you', 'ever', 'each', 'hundred', 'if', 'sixty', \"'m\", 'since', 'while', 'doing', 'by', 'is', 'around', 'themselves', 'may', 'thence', 'others', 'those', 'therein', 'becoming', 'are', 'become', 'give', 'nobody', 'than', 'throughout', 'mine', 'in', 'empty', 'whenever', \"'ve\", 'his', 'everywhere', 'get', 'nothing', \"'ll\", 'thereby', 'been', 'mostly', 'front', 'thereafter', 'well', 'its', 'must', 'be', 'even', 'namely', 'seem', 'n‘t', 'anyone', 'us', 'meanwhile', 'nevertheless'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for punctuations and stop words\n",
        "\n",
        "print(f\"{'Token':<20} {'Is stopword':<15} Is Punctuation\\n{'='*55}\")\n",
        "\n",
        "for token in doc:\n",
        "    stop = token.is_stop\n",
        "    punct = token.is_punct\n",
        "    print(f\"{token.text:<20} {stop:<15} {punct}\")"
      ],
      "metadata": {
        "id": "PntYPVjkVOfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c35b853-2257-407a-bc69-fa0cd2fe4845"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token                Is stopword     Is Punctuation\n",
            "=======================================================\n",
            "Apple                0               False\n",
            "is                   1               False\n",
            "looking              0               False\n",
            "at                   1               False\n",
            "buying               0               False\n",
            "U.K.                 0               False\n",
            "startup              0               False\n",
            "for                  1               False\n",
            "$                    0               False\n",
            "1                    0               False\n",
            "billion              0               False\n",
            ".                    0               True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_token(token):\n",
        "\n",
        "    \"\"\" If a token is not a stop word and not a punctuation,\n",
        "        then reduce it to its base form, remove trailing spaces, and covert to lowercase. \"\"\"\n",
        "\n",
        "    if not token.is_stop and not token.is_punct:\n",
        "        return token.lemma_.strip().lower()"
      ],
      "metadata": {
        "id": "11yH1FR4XAdq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "\n",
        "print(f\"{'Token':<20} Preprocessed token\\n{'='*40}\")\n",
        "\n",
        "for token in doc:\n",
        "    output = preprocess_token(token)\n",
        "    print(f\"{token.text:<20} {output}\")"
      ],
      "metadata": {
        "id": "ZoCOU8gSXz_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc47a3d-1287-49ba-ab1d-1d6a9cca9b92"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token                Preprocessed token\n",
            "========================================\n",
            "Apple                apple\n",
            "is                   None\n",
            "looking              look\n",
            "at                   None\n",
            "buying               buy\n",
            "U.K.                 u.k.\n",
            "startup              startup\n",
            "for                  None\n",
            "$                    $\n",
            "1                    1\n",
            "billion              billion\n",
            ".                    None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyeCdI6iK3Nz"
      },
      "source": [
        "### Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv2U9VEzK3N0"
      },
      "source": [
        "Named entity recognition (NER) is the task of recognising and classifying entities named in a text.\n",
        "\n",
        "spaCy can recognise the named entities such as persons, geographic locations, and products as these were annotated in the dataset its trained on (OntoNotes 5 corpus).\n",
        "\n",
        "We can use the *Doc* object's `.ents` attribute to get the named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9wn523SiK3N0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3686bd7c-5e17-4891-8408-e8b73b50097f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Apple, U.K., $1 billion)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Entities\n",
        "doc.ents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This returns a tuple with the named entities.\n",
        "\n",
        "Each item in the tuple is a spaCy *Span* object. *Span* objects can consist of multiple *Token* objects, as many named entities span multiple *Tokens*."
      ],
      "metadata": {
        "id": "oM4MeyX2qlas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the type of the object used to store named entities\n",
        "type(doc.ents[0])"
      ],
      "metadata": {
        "id": "NGs7oHNjqMGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1cc102-3176-49ec-c9b3-3a87a5363a6c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaCzY90CK3N0"
      },
      "source": [
        "The named entities and their types are stored under the attributes `.text` and `.label_` of each *Span* object.\n",
        "\n",
        "Let's loop over the *Span* objects in the tuple and print out both attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yveeIWxXK3N0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d22f50-c7dc-4949-90f7-c6da2d0fb838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text                 Entity_label     Explanation\n",
            "================================================================================\n",
            "Apple                ORG              Companies, agencies, institutions, etc.\n",
            "U.K.                 GPE              Countries, cities, states\n",
            "$1 billion           MONEY            Monetary values, including unit\n"
          ]
        }
      ],
      "source": [
        "# Loop over the named entities in the Doc object, and print the named entity and its label\n",
        "\n",
        "print(f\"{'Text':<20} {'Entity_label':<16} Explanation\\n{'='*80}\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    ent_text = ent.text           # named entity\n",
        "    ent_label = ent.label_        # entity label\n",
        "    ent_label_val = spacy.explain(ent_label)       # entity label explanation\n",
        "\n",
        "    print(f\"{ent_text:<20} {ent_label:<16} {ent_label_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GCgHkJ7K3N1"
      },
      "source": [
        "As you can see, named entities like '$1 billion' identified in the *Doc* consist of multiple *Tokens*, which is why they are represented as *Span* objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGjkG8m3K3N2"
      },
      "source": [
        "spaCy [*Span*](https://spacy.io/api/span) objects contain several useful arguments.\n",
        "\n",
        "Most importantly, the attributes `start` and `end` return the indices of _Tokens_, which determine where the _Span_ starts and ends in the *Doc* object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kPwNfX-VK3N2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea42221-970b-48f9-d94e-081eeb29eec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$1 billion 8 11\n"
          ]
        }
      ],
      "source": [
        "# Print the named entity and indices of its start and end Tokens\n",
        "print(doc.ents[2], doc.ents[2].start, doc.ents[2].end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z29gu0lwK3N3"
      },
      "source": [
        "The named entity starts at index 8 and ends at index 11 in the *Doc* object."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize Named Entities"
      ],
      "metadata": {
        "id": "3ycEVv9KrNkZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTKE_TnYK3N4"
      },
      "source": [
        "We can also render the named entities using *displacy*, the spaCy module we used for visualising dependency parses above.\n",
        "\n",
        "Note that we must pass the string `ent` to the `style` argument to indicate that we wish to visualise named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lUN9_a2KK3N5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "06aa0996-f77b-4ab1-987f-10cb7ccb4119"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is looking at buying \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " startup for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualize named entity\n",
        "spacy.displacy.render(doc, style='ent')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test another example 2:**"
      ],
      "metadata": {
        "id": "I5fLOBFpxSt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize another sample text\n",
        "text2 = \"On 3rd Feb, Ram was in Delhi.\\nLater he traveled to Mumbai via Air India flight reading a Time magazine to meet Raj.\\nAfter 10 days, he went again back to Delhi wearing a Timex watch.\"\n",
        "doc2 = nlp(text2)\n",
        "spacy.displacy.render(doc2, style='ent')"
      ],
      "metadata": {
        "id": "kZLngerFvbL7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a69efff1-18b6-4cd2-8a56-6f0dad8fba0a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">On \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    3rd Feb\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ram\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Delhi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".<br>Later he traveled to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mumbai\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " via \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Air India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " flight reading a \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Time\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " magazine to meet \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Raj\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ".<br>After \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    10 days\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", he went again back to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Delhi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " wearing a \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Timex\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " watch.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a particular tag used for a named entity is unfamiliar, you can check it's explanation."
      ],
      "metadata": {
        "id": "ogdrSb382R3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('DATE')"
      ],
      "metadata": {
        "id": "D-dFic41xMda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b7295759-8177-43b2-b1d0-c507bc9067c2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Absolute or relative dates or periods'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('PERSON')"
      ],
      "metadata": {
        "id": "IBp1zyMXxnJ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe058a26-eda7-42b1-9ee6-b2b9e3715415"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'People, including fictional'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test another example 3:**"
      ],
      "metadata": {
        "id": "XAvWA3882Ues"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize another sample text\n",
        "text3 = \"Holmes solves his another case while sitting at his home in Baker Street, without moving a single inch.\"\n",
        "doc3 = nlp(text3)\n",
        "spacy.displacy.render(doc3, style='ent')"
      ],
      "metadata": {
        "id": "UV6e_njh1LKX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "30dfcf00-6f1e-4df7-a8df-28f3ee1f6ed7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Holmes\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " solves his another case while sitting at his home in \n",
              "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Baker Street\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
              "</mark>\n",
              ", without moving \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    a single inch\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('FAC')"
      ],
      "metadata": {
        "id": "L76wtVpb1n6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74498f6c-1e60-4cf4-fbe8-ffec7202c36e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Buildings, airports, highways, bridges, etc.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "* https://spacy.io/usage/spacy-101"
      ],
      "metadata": {
        "id": "YFXGM68ARzhb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title Which of the following is a technique to convert a word into its base form? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"Lemmatization\" #@param [\"\", \"Tokenization\", \"Part-of-Speech Tagging\", \"Lemmatization\", \"Named Entity Recognition\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"na\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158b3d80-7608-436e-ba35-000c60e5d296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 5774\n",
            "Date of submission:  12 Aug 2024\n",
            "Time of submission:  22:25:13\n",
            "View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ]
}